{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis notebook runs the CEBRA analysis using a \"Switch/Stay\" label for the dynamic foraging task. Here \"switch\" refers to whether or not the mouse switched sides in a given trial.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook runs the CEBRA analysis using a \"Switch/Stay\" label for the dynamic foraging task. Here \"switch\" refers to whether or not the mouse switched sides in a given trial.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/cebra/datasets/monkey_reaching.py:35: UserWarning: Could not import the nlb_tools package required for data loading of cebra.datasets.monkey_reaching. Dataset will not be available. If required, you can install the dataset by running pip install git+https://github.com/neurallatents/nlb_tools.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os # my addtion\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.integrate import solve_ivp\n",
    "import cebra.data\n",
    "import torch\n",
    "import cebra.integrations\n",
    "import cebra.datasets\n",
    "from cebra import CEBRA\n",
    "import torch\n",
    "import pickle\n",
    "import cebra_pack.cebra_utils as cp\n",
    "\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Load the Data\n",
    "\n",
    "Here we load data from the Fibre Photometry pipeline of 4 Neuromodulators (DA, 5HT, ACh, NE) recorded in the Nucleus Acumbens region. The main neural data will be in the form of dF_F traces of these 4 Neuromodulators (NMs). These will be stored in a 2D array, 'all_nms'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe that contains data from 1 session\n",
    "df_trials_ses = pickle.load(open('../data/CO data/df.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bit_code', 'ses_idx', 'rpe', 'left_action_value', 'right_action_value',\n",
       "       'licks L', 'licks R', 'Lick L (raw)', 'Lick R (raw)', 'trial', 'reward',\n",
       "       'choice', 'go_cue_absolute_time', 'go_cue', 'choice_time',\n",
       "       'reward_time', 'onset', 'NM', 'NM_name', 'region', 'last_value_NM',\n",
       "       'overlap_index', 'NM_no_overlap', 'bins_mids', 'bins_mids_no_overlap'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trials_ses.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dictionary containing the traces\n",
    "traces = pickle.load(open('../data/CO data/traces.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trace times\n",
    "trace_times = np.load('../data/CO data/Trace times.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the choice time \n",
    "choice_times = df_trials_ses['choice_time'][0:n_trials].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218572, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the traces into one 2D array\n",
    "all_nms = np.array([traces[trace] for trace in traces.keys()])\n",
    "all_nms = np.transpose(all_nms)\n",
    "\n",
    "# change it to an array of floats (previously it was an array of object datatype)\n",
    "all_nms_new = all_nms.astype(np.float64)\n",
    "all_nms_new.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218572, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218572, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change it to an array of floats (previously it was an array of object datatype)\n",
    "all_nms_new = all_nms.astype(np.float64)\n",
    "all_nms_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([218572, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert it to a tensor (this is probably not necessary but we want it to be as close to the inputs in the previous notebook)\n",
    "all_nms_tensor = torch.from_numpy(all_nms_new)\n",
    "all_nms_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Format data and create the behavioural/auxiliary variables\n",
    "\n",
    "Now let's format the data. We want to view the data in a 1 second window around the choice time at each trial in the session. The hope is that this will make it easy to identify the trials where it chose to lick left and those where it chose to lick right.\n",
    "\n",
    "Each trial will be labelled as rewarded/unrewarded and this will be the behavioural variable we use for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = np.argwhere(df_trials_ses['licks R'] > df_trials_ses['licks L'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26., 10.,  2., ...,  2.,  1.,  1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trials_ses['licks L'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the trials where the number of left and right trials are equal, the number of licks in either direction is either 0,1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to format the NM data into a 1s window around the choice\n",
    "def format_data(neural_data, df, trace_times_, choice_times_ , window=None , window_size=10):\n",
    "\n",
    "    # define the number of trials where the mouse made a choice\n",
    "    n_choice_trials = np.unique(np.isnan(choice_times_),return_counts=True)[1][0]\n",
    "\n",
    "    # define total number of trials\n",
    "    n_total_trials = np.sum(np.unique(np.isnan(choice_times_),return_counts=True)[1])\n",
    "\n",
    "    # list to hold all the 1s windows\n",
    "    n_data_window = []\n",
    "\n",
    "    # new labels\n",
    "    reward_labels = []\n",
    "    choice_labels = []\n",
    "    rpe_labels = []\n",
    "    n_licks = []\n",
    "\n",
    "\n",
    "    # loop over all trials\n",
    "    for i in range(0,n_total_trials):\n",
    "\n",
    "        # skip trials where the animal didn't make a choice (null choice time)\n",
    "        if np.isnan(choice_times_[i]):\n",
    "            continue\n",
    "\n",
    "        # find the index of the closest time to the choice time in the trace_times array \n",
    "        idx = np.abs(trace_times_ - choice_times_[i]).argmin()\n",
    "\n",
    "        # take the previous 10 and/or the next 10 values of the NM data at these indices - 1s window\n",
    "        if window =='before':\n",
    "            n_data_window.append(neural_data[idx-10:idx])\n",
    "\n",
    "        if window == 'after':\n",
    "            n_data_window.append(neural_data[idx:idx+10])\n",
    "\n",
    "        if window == None:\n",
    "            n_data_window.append(neural_data[idx-10:idx+10])\n",
    "\n",
    "        # label the timepoints as rewarded or unrewarded\n",
    "        if df['reward'].iloc[i]:\n",
    "            # new trial label\n",
    "            reward_labels.append(1)\n",
    "\n",
    "        elif df['reward'].iloc[i]==False:\n",
    "            # new trial label\n",
    "            reward_labels.append(0)\n",
    "        \n",
    "        # label the timepoints as left or right choice\n",
    "        if df['licks L'].iloc[i] >= df['licks R'].iloc[i]:\n",
    "            # new trial label\n",
    "            choice_labels.append(1)\n",
    "            n_licks.append(df['licks L'].iloc[i])\n",
    "\n",
    "        elif df['licks R'].iloc[i] > df['licks L'].iloc[i]:\n",
    "            # new trial label\n",
    "            choice_labels.append(0)\n",
    "            n_licks.append(df['licks R'].iloc[i])\n",
    "\n",
    "        # get the rpe values at each trial\n",
    "        rpe_labels.append(df['rpe'].iloc[i])\n",
    "\n",
    "    # stack the nm data for each trial\n",
    "    nms_HD = np.stack(n_data_window).reshape((n_choice_trials,-1))\n",
    "    # format it into a tensor\n",
    "    nms_HD = torch.from_numpy(nms_HD.astype(np.float64))\n",
    "    print(\"neural tensor shape: \", nms_HD.shape)\n",
    "\n",
    "    # convert trial labels into an array\n",
    "    reward_labels = np.array(reward_labels)\n",
    "    print(\"reward labels shape: \",reward_labels.shape)\n",
    "\n",
    "    choice_labels = np.array(choice_labels)\n",
    "    print(\"choice labels shape: \",choice_labels.shape)\n",
    "\n",
    "    # convert rpe labels to arrays\n",
    "    rpe_labels = np.array(rpe_labels)\n",
    "    print(\"rpe labels shape:\", rpe_labels.shape)\n",
    "\n",
    "\n",
    "    return nms_HD, reward_labels, choice_labels, rpe_labels, n_licks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural tensor shape:  torch.Size([1717, 80])\n",
      "reward labels shape:  (1717,)\n",
      "choice labels shape:  (1717,)\n",
      "rpe labels shape: (1717,)\n"
     ]
    }
   ],
   "source": [
    "formatted_nms, reward_labels, choice_labels, rpe_labels, n_licks = format_data(all_nms,df=df_trials_ses,trace_times_=trace_times, choice_times_=choice_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to take the choice labels and make a 'Switch' label\n",
    "\n",
    "def make_switch_label(choice_label):\n",
    "\n",
    "    # make sure input is in array form\n",
    "    assert type(choice_label)==np.ndarray\n",
    "\n",
    "    switch_labels = []\n",
    "\n",
    "    for i in range(0,choice_label.shape[0]):\n",
    "\n",
    "        # should I just skip this first one?\n",
    "        if i==0:\n",
    "            switch_labels.append(0)\n",
    "            continue\n",
    "\n",
    "        # make switch label based on previous trial\n",
    "        if choice_label[i]!=choice_label[i-1]:\n",
    "            switch_labels.append(1)        \n",
    "        \n",
    "        elif choice_label[i]==choice_label[i-1]:\n",
    "            switch_labels.append(0)\n",
    "\n",
    "    switch_labels = np.array(switch_labels)\n",
    "    print('Switch labels shape:', switch_labels.shape)\n",
    "\n",
    "    return switch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch labels shape: (1717,)\n"
     ]
    }
   ],
   "source": [
    "switch_labels = make_switch_label(choice_label=choice_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Build and train the CEBRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the maximum number of iterations for training the model\n",
    "max_iterations = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a CEBRA-Time and CEBRA-Behaviour model\n",
    "cebra_time_model = CEBRA(model_architecture='offset10-model-mse',\n",
    "                        batch_size=512,\n",
    "                        learning_rate=3e-4,\n",
    "                        temperature=1,\n",
    "                        output_dimension=3,\n",
    "                        max_iterations=max_iterations,\n",
    "                        distance='euclidean',\n",
    "                        conditional='time',\n",
    "                        device='cuda_if_available',\n",
    "                        verbose=True,\n",
    "                        time_offsets=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cebra_behaviour_model = CEBRA(model_architecture='offset10-model-mse',\n",
    "                        batch_size=512,\n",
    "                        learning_rate=3e-4,\n",
    "                        temperature=1,\n",
    "                        output_dimension=3,\n",
    "                        max_iterations=max_iterations,\n",
    "                        distance='euclidean',\n",
    "                        conditional='time_delta',\n",
    "                        device='cuda_if_available',\n",
    "                        verbose=True,\n",
    "                        time_offsets=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.9/site-packages/torch/nn/modules/conv.py:309: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "pos:  0.8830 neg:  2.9669 total:  3.8499 temperature:  1.0000: 100%|██████████| 2000/2000 [00:23<00:00, 85.06it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CEBRA(batch_size=512, conditional=&#x27;time&#x27;, distance=&#x27;euclidean&#x27;,\n",
       "      max_iterations=2000, model_architecture=&#x27;offset10-model-mse&#x27;,\n",
       "      output_dimension=3, temperature=1, time_offsets=10, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CEBRA</label><div class=\"sk-toggleable__content\"><pre>CEBRA(batch_size=512, conditional=&#x27;time&#x27;, distance=&#x27;euclidean&#x27;,\n",
       "      max_iterations=2000, model_architecture=&#x27;offset10-model-mse&#x27;,\n",
       "      output_dimension=3, temperature=1, time_offsets=10, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CEBRA(batch_size=512, conditional='time', distance='euclidean',\n",
       "      max_iterations=2000, model_architecture='offset10-model-mse',\n",
       "      output_dimension=3, temperature=1, time_offsets=10, verbose=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the time model (no labels here)\n",
    "cebra_time_model.fit(formatted_nms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos:  0.1595 neg:  6.0460 total:  6.2055 temperature:  1.0000: 100%|██████████| 2000/2000 [00:16<00:00, 124.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CEBRA(batch_size=512, conditional=&#x27;time_delta&#x27;, distance=&#x27;euclidean&#x27;,\n",
       "      max_iterations=2000, model_architecture=&#x27;offset10-model-mse&#x27;,\n",
       "      output_dimension=3, temperature=1, time_offsets=10, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CEBRA</label><div class=\"sk-toggleable__content\"><pre>CEBRA(batch_size=512, conditional=&#x27;time_delta&#x27;, distance=&#x27;euclidean&#x27;,\n",
       "      max_iterations=2000, model_architecture=&#x27;offset10-model-mse&#x27;,\n",
       "      output_dimension=3, temperature=1, time_offsets=10, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CEBRA(batch_size=512, conditional='time_delta', distance='euclidean',\n",
       "      max_iterations=2000, model_architecture='offset10-model-mse',\n",
       "      output_dimension=3, temperature=1, time_offsets=10, verbose=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the behaviour model (use the labels here)\n",
    "cebra_behaviour_model.fit(formatted_nms, switch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Compute and view embeddings\n",
    "\n",
    "Here, we compute the embeddings from the two trained models and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_embedding = cebra_time_model.transform(formatted_nms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour_embedding = cebra_behaviour_model.transform(formatted_nms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the labels into right and left\n",
    "switch = switch_labels==1\n",
    "stay = switch_labels==0\n",
    "\n",
    "switch = switch.flatten()\n",
    "stay = stay.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6963423649035996, 0.5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get auc scores\n",
    "mean_scores, errors = cp.get_auc([behaviour_embedding, time_embedding], trial_labels=switch_labels)\n",
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 0.5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(mean_scores,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'latent 3')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a figure and make the plots\n",
    "fig1 = plt.figure(figsize=(16,4))\n",
    "gs = gridspec.GridSpec(1, 2, figure=fig1)\n",
    "\n",
    "ax1 = fig1.add_subplot(gs[0,0], projection='3d')\n",
    "ax2 = fig1.add_subplot(gs[0,1], projection='3d')\n",
    "axes =[ax1,ax2]\n",
    "\n",
    "for ax in axes:\n",
    "\n",
    "\n",
    "    ax.set_xlabel(\"latent 1\", labelpad=0.01)\n",
    "    ax.set_ylabel(\"latent 2\", labelpad=0.01)\n",
    "    ax.set_zlabel(\"latent 3\", labelpad=0.01)\n",
    "\n",
    "    # Hide X and Y axes label marks\n",
    "    ax.xaxis.set_tick_params(labelbottom=False)\n",
    "    ax.yaxis.set_tick_params(labelleft=False)\n",
    "    ax.zaxis.set_tick_params(labelright=False)\n",
    "\n",
    "    # Hide X and Y axes tick marks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "\n",
    "\n",
    "# colour maps\n",
    "colours = ['cool', 'plasma']\n",
    "\n",
    "# Aucs rounded\n",
    "mean_scores = np.round(mean_scores,2)\n",
    "\n",
    "# plot the time embedding \n",
    "cebra.plot_embedding(embedding=time_embedding[switch,:], embedding_labels=switch_labels[switch],ax=ax1, markersize=2, title='Time embedding', cmap=colours[0])\n",
    "cebra.plot_embedding(embedding=time_embedding[stay,:], embedding_labels=switch_labels[stay],ax=ax1, markersize=2, title='Time embedding, AUC:{}'.format(mean_scores[1]), cmap=colours[1])\n",
    "\n",
    "# plot the behaviour embedding \n",
    "cebra.plot_embedding(embedding=behaviour_embedding[switch,:], embedding_labels=switch_labels[switch],ax=ax2, markersize=2, title='Behaviour embedding', cmap=colours[0])\n",
    "cebra.plot_embedding(embedding=behaviour_embedding[stay,:], embedding_labels=switch_labels[stay],ax=ax2,markersize=2, title='Behaviour embedding, AUC: {}'.format(mean_scores[0]),  cmap=colours[1])\n",
    "\n",
    "\n",
    "# Adjust the subplot layout manually\n",
    "#plt.subplots_adjust(left=0.095, right=0.1, top=0.95, bottom=0.05, wspace=0.001)\n",
    "\n",
    "# Adjust the subplot layout manually\n",
    "plt.subplots_adjust(left=0.00001, right=0.55, top=0.95, bottom=0.05, wspace=0.0001)\n",
    "\n",
    "# Use tight_layout with padding to ensure labels are not cut off\n",
    "\n",
    "\n",
    "# Adjust label positions using bbox\n",
    "ax2.set_zlabel(\"latent 3\", labelpad=1, fontsize=10, bbox=dict(facecolor='white', edgecolor='none', pad=0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([1441,  276]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(switch_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos:  0.0592 neg:  5.4446 total:  5.5038 temperature:  1.0000: 100%|██████████| 2000/2000 [00:14<00:00, 136.60it/s]\n",
      "pos:  0.2666 neg:  5.8733 total:  6.1399 temperature:  1.0000: 100%|██████████| 2000/2000 [00:17<00:00, 114.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# build, train and compute with the time and behaviour models with this new labels\n",
    "t_embed,b_embed =  cp.build_train_compute(formatted_nms,switch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to view the embeddings\n",
    "def view(time_embedding, behaviour_embedding, labels, label_classes, size=5):\n",
    " \n",
    "    # create a figure and make the plots\n",
    "    fig = plt.figure(figsize=(14,8))\n",
    "    gs = gridspec.GridSpec(1, 2, figure=fig)\n",
    "\n",
    "\n",
    "    ax81 = fig.add_subplot(gs[0,0], projection='3d')\n",
    "    ax82 = fig.add_subplot(gs[0,1], projection='3d')\n",
    " \n",
    "\n",
    "    # colour maps\n",
    "    colours = ['cool', 'plasma', 'spring']\n",
    "\n",
    "    # plot the time embedding \n",
    "    cebra.plot_embedding(embedding=time_embedding[label_classes[1],:], embedding_labels=labels[label_classes[1]],ax=ax81, markersize=size, title='Time embedding', cmap=colours[1])\n",
    "    cebra.plot_embedding(embedding=time_embedding[label_classes[0],:], embedding_labels=labels[label_classes[0]],ax=ax81, markersize=size, title='Time embedding', cmap=colours[0])\n",
    "\n",
    "\n",
    "    # plot the behaviour embedding \n",
    "    cebra.plot_embedding(embedding=behaviour_embedding[label_classes[1],:], embedding_labels=labels[label_classes[1]],ax=ax82, markersize=size, title='Behaviour embedding', cmap=colours[1])\n",
    "    cebra.plot_embedding(embedding=behaviour_embedding[label_classes[0],:], embedding_labels=labels[label_classes[0]],ax=ax82,markersize=size, title='Behaviour embedding',  cmap=colours[0])\n",
    "\n",
    "    gs.tight_layout(figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "view(t_embed, b_embed,switch_labels, [switch,stay])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try this with the infoNCE setting -- maybe the behaviour embedding will make sense then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of formatted array: (218572, 1)\n",
      "shape of formatted array: (218572, 1)\n",
      "shape of formatted array: (218572, 1)\n",
      "shape of formatted array: (218572, 1)\n"
     ]
    }
   ],
   "source": [
    "individual_nms = cp.individual_datasets(traces_=traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural tensor shape:  torch.Size([1717, 20])\n",
      "reward labels shape:  (1717,)\n",
      "choice labels shape:  (1717,)\n",
      "rpe labels shape: (1717,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos:  0.1838 neg:  5.6258 total:  5.8096 temperature:  1.0000: 100%|██████████| 2000/2000 [00:14<00:00, 142.23it/s]\n",
      "pos:  0.0987 neg:  6.1227 total:  6.2214 temperature:  1.0000: 100%|██████████| 2000/2000 [00:17<00:00, 117.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED ANALYSIS OF NM 0: \n",
      "neural tensor shape:  torch.Size([1717, 20])\n",
      "reward labels shape:  (1717,)\n",
      "choice labels shape:  (1717,)\n",
      "rpe labels shape: (1717,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos:  0.1095 neg:  5.4901 total:  5.5995 temperature:  1.0000: 100%|██████████| 2000/2000 [00:13<00:00, 145.55it/s]\n",
      "pos:  0.1308 neg:  6.1011 total:  6.2319 temperature:  1.0000: 100%|██████████| 2000/2000 [00:17<00:00, 114.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED ANALYSIS OF NM 1: \n",
      "neural tensor shape:  torch.Size([1717, 20])\n",
      "reward labels shape:  (1717,)\n",
      "choice labels shape:  (1717,)\n",
      "rpe labels shape: (1717,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos:  0.3448 neg:  5.4783 total:  5.8231 temperature:  1.0000: 100%|██████████| 2000/2000 [00:13<00:00, 147.89it/s]\n",
      "pos:  0.0841 neg:  6.1592 total:  6.2433 temperature:  1.0000: 100%|██████████| 2000/2000 [00:16<00:00, 118.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED ANALYSIS OF NM 2: \n",
      "neural tensor shape:  torch.Size([1717, 20])\n",
      "reward labels shape:  (1717,)\n",
      "choice labels shape:  (1717,)\n",
      "rpe labels shape: (1717,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos:  0.1471 neg:  5.4739 total:  5.6210 temperature:  1.0000: 100%|██████████| 2000/2000 [00:15<00:00, 129.77it/s]\n",
      "pos:  0.0001 neg:  6.2382 total:  6.2383 temperature:  1.0000: 100%|██████████| 2000/2000 [00:15<00:00, 131.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED ANALYSIS OF NM 3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "b_embeds, t_embeds, sw_labels, [switch, stay] = cp.nm_analysis_2(individual_nms, df_trials_ses, trace_times, choice_times, title=\" \",other_label=switch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "means, sds = cp.get_auc(b_embeds, sw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5794763097285499, 0.6290430860211809, 0.546714238300697, 0.5]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make function to make the plots given a list of embeddings\n",
    "def plot4_embeddings(embeddings, labels , l_class, means=None, titles=['DA only', 'NE only', '5HT only', 'ACh only'], t=\"\"):\n",
    "\n",
    "    # number of plots\n",
    "    n_plots = len(embeddings)\n",
    "\n",
    "    n_columns = 2\n",
    "    n_rows = n_plots//n_columns\n",
    "\n",
    "    # create axis\n",
    "    fig = plt.figure(figsize=(8,4*n_plots))\n",
    "    gs = gridspec.GridSpec(n_rows, n_columns, figure=fig)\n",
    "\n",
    "    # colour \n",
    "    c = ['cool','plasma','pink','winter']\n",
    "\n",
    "    for i, embed in enumerate(embeddings):\n",
    "\n",
    "        # create the axes\n",
    "        ax = fig.add_subplot(gs[i // n_columns, i%n_columns], projection='3d')\n",
    "\n",
    "        ax.set_xlabel(\"latent 1\", labelpad=0.001, fontsize=13)\n",
    "        ax.set_ylabel(\"latent 2\", labelpad=0.001, fontsize=13)\n",
    "        ax.set_zlabel(\"latent 3\", labelpad=0.001, fontsize=13)\n",
    "\n",
    "        # Hide X and Y axes label marks\n",
    "        ax.xaxis.set_tick_params(labelbottom=False)\n",
    "        ax.yaxis.set_tick_params(labelleft=False)\n",
    "        ax.zaxis.set_tick_params(labelright=False)\n",
    "\n",
    "        # Hide X and Y axes tick marks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "\n",
    "\n",
    "        if means.any():\n",
    "            titles=['DA only, AUC:{}'.format(means[0]),'NE only, AUC:{}'.format(means[1]), '5HT only, AUC:{}'.format(means[2]), 'ACh only, AUC:{}'.format(means[3])]\n",
    "\n",
    "\n",
    "        # plot the embedding\n",
    "        cebra.plot_embedding(embedding=embed[l_class[0],:], embedding_labels=labels[l_class[0]], ax=ax, markersize=2,title=titles[i], cmap=c[0])\n",
    "        cebra.plot_embedding(embedding=embed[l_class[1],:], embedding_labels=labels[l_class[1]], ax=ax, markersize=2,title=titles[i], cmap=c[1])\n",
    "\n",
    "    plt.suptitle(t, fontsize=15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58, 0.63, 0.55, 0.5 ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(means,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "plot4_embeddings(b_embeds, sw_labels, [switch, stay], means=np.round(means,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
